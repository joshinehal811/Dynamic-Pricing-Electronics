{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\sahil\\AppData\\Local\\Temp\\ipykernel_13884\\4128556153.py:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  df = pd.read_csv('C:\\PADHAI MMA FALL SEM\\MMA\\Winter sem 2\\Fatih ML 2\\ML_data.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\PADHAI MMA FALL SEM\\MMA\\Winter sem 2\\Fatih ML 2\\ML_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-24 11:57:06 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1996170</td>\n",
       "      <td>2144415922528452715</td>\n",
       "      <td>electronics.telephone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.90</td>\n",
       "      <td>1515915625519388267</td>\n",
       "      <td>LJuJVLEjPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-24 11:57:26 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>139905</td>\n",
       "      <td>2144415926932472027</td>\n",
       "      <td>computers.components.cooler</td>\n",
       "      <td>zalman</td>\n",
       "      <td>17.16</td>\n",
       "      <td>1515915625519380411</td>\n",
       "      <td>tdicluNnRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-24 11:57:27 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>215454</td>\n",
       "      <td>2144415927158964449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.81</td>\n",
       "      <td>1515915625513238515</td>\n",
       "      <td>4TMArHtXQy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-24 11:57:33 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>635807</td>\n",
       "      <td>2144415923107266682</td>\n",
       "      <td>computers.peripherals.printer</td>\n",
       "      <td>pantum</td>\n",
       "      <td>113.81</td>\n",
       "      <td>1515915625519014356</td>\n",
       "      <td>aGFYrNgC08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-24 11:57:36 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>3658723</td>\n",
       "      <td>2144415921169498184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cameronsino</td>\n",
       "      <td>15.87</td>\n",
       "      <td>1515915625510743344</td>\n",
       "      <td>aa4mmk0kwQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time event_type  product_id          category_id  \\\n",
       "0  2020-09-24 11:57:06 UTC       view     1996170  2144415922528452715   \n",
       "1  2020-09-24 11:57:26 UTC       view      139905  2144415926932472027   \n",
       "2  2020-09-24 11:57:27 UTC       view      215454  2144415927158964449   \n",
       "3  2020-09-24 11:57:33 UTC       view      635807  2144415923107266682   \n",
       "4  2020-09-24 11:57:36 UTC       view     3658723  2144415921169498184   \n",
       "\n",
       "                   category_code        brand   price              user_id  \\\n",
       "0          electronics.telephone          NaN   31.90  1515915625519388267   \n",
       "1    computers.components.cooler       zalman   17.16  1515915625519380411   \n",
       "2                            NaN          NaN    9.81  1515915625513238515   \n",
       "3  computers.peripherals.printer       pantum  113.81  1515915625519014356   \n",
       "4                            NaN  cameronsino   15.87  1515915625510743344   \n",
       "\n",
       "  user_session  \n",
       "0   LJuJVLEjPT  \n",
       "1   tdicluNnRY  \n",
       "2   4TMArHtXQy  \n",
       "3   aGFYrNgC08  \n",
       "4   aa4mmk0kwQ  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                           +-----------------+\n",
    "                           | Raw Events Table |  (Bronze Layer)\n",
    "                           +-----------------+\n",
    "                                   ↓\n",
    "                      Session Aggregation, Feature Engineering\n",
    "                                   ↓\n",
    "                           +-----------------+\n",
    "                           | Aggregated Data  |  (Silver Layer)\n",
    "                           +-----------------+\n",
    "                                   ↓\n",
    "         +---------------------+         +--------------------------+\n",
    "         | Gold Table for       |         | Gold Table for            |\n",
    "         | Multi-Class Action   |         | Customer Behavior         |\n",
    "         +---------------------+         +--------------------------+\n",
    "                 ↓                                    ↓\n",
    "       Train Multi-Class Model                 Train Clustering Model\n",
    "      (TabNet, XGBoost, CatBoost)              (KMeans, HDBSCAN Clustering)\n",
    "                 ↓                                    ↓\n",
    "       Train Meta-Classifier                     Train Classifier on Clusters\n",
    "     (Stacked Ensemble)                          (optional supervised learning)\n",
    "                 ↓                                    ↓\n",
    "        MLflow Model Registry                    MLflow Model Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\AppData\\Local\\Temp\\ipykernel_13884\\848689186.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['brand'].fillna('unknown', inplace=True)\n",
      "C:\\Users\\sahil\\AppData\\Local\\Temp\\ipykernel_13884\\848689186.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['category_code'].fillna('unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load and basic clean\n",
    "df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "df['brand'].fillna('unknown', inplace=True)\n",
    "df['category_code'].fillna('unknown', inplace=True)\n",
    "df = df[df['price'] > 0]\n",
    "\n",
    "# Sort events\n",
    "df = df.sort_values(['user_id', 'event_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['next_event'] = df.groupby('user_id')['event_type'].shift(-1)\n",
    "\n",
    "# Label = Last event_type in session.\n",
    "# If using custom sessions: create session_id, and for each session, pick the last event's event_type as label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      event_time event_type  product_id          category_id  \\\n",
      "174670 2020-10-29 11:28:35+00:00       view     4101974  2144415939364389423   \n",
      "51155  2020-10-06 06:30:32+00:00       view     3506650  2144415935673401802   \n",
      "242211 2020-11-09 08:52:51+00:00       view      124883  2144415924424278172   \n",
      "242297 2020-11-09 09:04:34+00:00       view      760044  2144415924424278172   \n",
      "242335 2020-11-09 09:08:53+00:00       view      125325  2144415924424278172   \n",
      "\n",
      "                     category_code     brand  price              user_id  \\\n",
      "174670          electronics.clocks     honor  76.48  1515915625353226922   \n",
      "51155                      unknown    kester  28.98  1515915625353230067   \n",
      "242211  electronics.audio.acoustic  logitech  23.90  1515915625353230683   \n",
      "242297  electronics.audio.acoustic      dell  48.73  1515915625353230683   \n",
      "242335  electronics.audio.acoustic  logitech  23.90  1515915625353230683   \n",
      "\n",
      "       user_session             session_id  \n",
      "174670   7qejzWzHlR  1515915625353226922_1  \n",
      "51155    ikPKHkuRhA  1515915625353230067_1  \n",
      "242211   dn9FkZ11dA  1515915625353230683_1  \n",
      "242297   dn9FkZ11dA  1515915625353230683_1  \n",
      "242335   dn9FkZ11dA  1515915625353230683_1  \n"
     ]
    }
   ],
   "source": [
    "# Time difference from previous event (per user)\n",
    "df['prev_event_time'] = df.groupby('user_id')['event_time'].shift(1)\n",
    "df['time_diff_minutes'] = (df['event_time'] - df['prev_event_time']).dt.total_seconds() / 60\n",
    "\n",
    "# New session if time_diff > 30 minutes or first event\n",
    "df['new_session'] = (df['time_diff_minutes'] > 30) | (df['time_diff_minutes'].isna())\n",
    "\n",
    "# Cumulative sum of new sessions to assign session_id\n",
    "df['session_id'] = df.groupby('user_id')['new_session'].cumsum()\n",
    "\n",
    "# Combine user_id + session_id to make global session_id\n",
    "df['session_id'] = df['user_id'].astype(str) + '_' + df['session_id'].astype(int).astype(str)\n",
    "\n",
    "# Drop helper columns\n",
    "df.drop(['prev_event_time', 'time_diff_minutes', 'new_session'], axis=1, inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 885129 entries, 174670 to 885127\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count   Dtype              \n",
      "---  ------         --------------   -----              \n",
      " 0   event_time     885129 non-null  datetime64[ns, UTC]\n",
      " 1   event_type     885129 non-null  object             \n",
      " 2   product_id     885129 non-null  int64              \n",
      " 3   category_id    885129 non-null  int64              \n",
      " 4   category_code  885129 non-null  object             \n",
      " 5   brand          885129 non-null  object             \n",
      " 6   price          885129 non-null  float64            \n",
      " 7   user_id        885129 non-null  int64              \n",
      " 8   user_session   884964 non-null  object             \n",
      " 9   session_id     885129 non-null  object             \n",
      " 10  next_event     477846 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), int64(3), object(6)\n",
      "memory usage: 81.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "Feature Engineering\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user:\n",
    "If time difference between consecutive events > 30 minutes → new session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\AppData\\Local\\Temp\\ipykernel_13884\\3699480669.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_agg['price__price_std'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Feature aggregation per session\n",
    "agg_funcs = {\n",
    "    'event_type': [\n",
    "        ('total_views', lambda x: (x == 'view').sum()),\n",
    "        ('total_carts', lambda x: (x == 'cart').sum()),\n",
    "        ('total_purchases', lambda x: (x == 'purchase').sum())\n",
    "    ],\n",
    "    'price': [\n",
    "        ('avg_price', 'mean'),\n",
    "        ('price_std', 'std'),\n",
    "    ],\n",
    "    'event_time': [\n",
    "        ('session_start', 'min'),\n",
    "        ('session_end', 'max'),\n",
    "        ('avg_hour', lambda x: x.dt.hour.mean())\n",
    "    ],\n",
    "    'category_code': [\n",
    "        ('most_freq_category', lambda x: x.mode()[0] if not x.mode().empty else 'unknown')\n",
    "    ],\n",
    "    'brand': [\n",
    "        ('most_freq_brand', lambda x: x.mode()[0] if not x.mode().empty else 'unknown')\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Reformatting for Pandas GroupBy\n",
    "agg_dict = {}\n",
    "for col, operations in agg_funcs.items():\n",
    "    for name, func in operations:\n",
    "        agg_dict[f\"{col}__{name}\"] = (col, func)\n",
    "\n",
    "# Perform aggregation\n",
    "df_agg = df.groupby('session_id').agg(**agg_dict).reset_index()\n",
    "\n",
    "# Post-processing\n",
    "df_agg['session_duration_secs'] = (df_agg['event_time__session_end'] - df_agg['event_time__session_start']).dt.total_seconds()\n",
    "df_agg['events_per_minute'] = (df_agg['event_type__total_views'] + df_agg['event_type__total_carts'] + df_agg['event_type__total_purchases']) / (df_agg['session_duration_secs'] / 60 + 1)\n",
    "\n",
    "# Ratios\n",
    "df_agg['view_to_cart_ratio'] = df_agg['event_type__total_carts'] / (df_agg['event_type__total_views'] + 1)\n",
    "df_agg['cart_to_purchase_ratio'] = df_agg['event_type__total_purchases'] / (df_agg['event_type__total_carts'] + 1)\n",
    "df_agg['view_to_purchase_ratio'] = df_agg['event_type__total_purchases'] / (df_agg['event_type__total_views'] + 1)\n",
    "\n",
    "# Fill missing std price\n",
    "df_agg['price__price_std'].fillna(0, inplace=True)\n",
    "\n",
    "# Rename cleanly\n",
    "df_agg.columns = [col.replace('event_type__', '').replace('price__', '').replace('event_time__', '') for col in df_agg.columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 1 row per session_id with engineered features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Label Creation for Classifier |\n",
    " \n",
    "Logic:\n",
    "\n",
    "Find the last event in each session.\n",
    "\n",
    "That event_type will be the label: view / cart / purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              session_id  total_views  total_carts  total_purchases  \\\n",
      "0  1515915625353226922_1            1            0                0   \n",
      "1  1515915625353230067_1            1            0                0   \n",
      "2  1515915625353230683_1            8            0                0   \n",
      "3  1515915625353230683_2            4            0                0   \n",
      "4  1515915625353230683_3            1            0                0   \n",
      "\n",
      "   avg_price  price_std             session_start               session_end  \\\n",
      "0    76.4800   0.000000 2020-10-29 11:28:35+00:00 2020-10-29 11:28:35+00:00   \n",
      "1    28.9800   0.000000 2020-10-06 06:30:32+00:00 2020-10-06 06:30:32+00:00   \n",
      "2    22.2725  12.484057 2020-11-09 08:52:51+00:00 2020-11-09 09:26:38+00:00   \n",
      "3    74.2400   0.000000 2020-11-18 10:51:35+00:00 2020-11-18 10:59:26+00:00   \n",
      "4   339.7900   0.000000 2020-12-12 10:33:09+00:00 2020-12-12 10:33:09+00:00   \n",
      "\n",
      "   avg_hour category_code__most_freq_category brand__most_freq_brand  \\\n",
      "0    11.000                electronics.clocks                  honor   \n",
      "1     6.000                           unknown                 kester   \n",
      "2     8.875        electronics.audio.acoustic               logitech   \n",
      "3    10.000        electronics.audio.acoustic               creative   \n",
      "4    10.000     computers.peripherals.printer                  canon   \n",
      "\n",
      "   session_duration_secs  events_per_minute  view_to_cart_ratio  \\\n",
      "0                    0.0           1.000000                 0.0   \n",
      "1                    0.0           1.000000                 0.0   \n",
      "2                 2027.0           0.229995                 0.0   \n",
      "3                  471.0           0.451977                 0.0   \n",
      "4                    0.0           1.000000                 0.0   \n",
      "\n",
      "   cart_to_purchase_ratio  view_to_purchase_ratio final_action  \n",
      "0                     0.0                     0.0         view  \n",
      "1                     0.0                     0.0         view  \n",
      "2                     0.0                     0.0         view  \n",
      "3                     0.0                     0.0         view  \n",
      "4                     0.0                     0.0         view  \n"
     ]
    }
   ],
   "source": [
    "# Get last event per session\n",
    "df_last_event = df.sort_values(['user_id', 'session_id', 'event_time']).groupby('session_id').tail(1)\n",
    "session_labels = df_last_event[['session_id', 'event_type']].rename(columns={'event_type': 'final_action'})\n",
    "# Merge label with features\n",
    "df_final = df_agg.merge(session_labels, on='session_id', how='left')\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 504679 entries, 0 to 504678\n",
      "Data columns (total 17 columns):\n",
      " #   Column                             Non-Null Count   Dtype              \n",
      "---  ------                             --------------   -----              \n",
      " 0   session_id                         504679 non-null  object             \n",
      " 1   total_views                        504679 non-null  int64              \n",
      " 2   total_carts                        504679 non-null  int64              \n",
      " 3   total_purchases                    504679 non-null  int64              \n",
      " 4   avg_price                          504679 non-null  float64            \n",
      " 5   price_std                          504679 non-null  float64            \n",
      " 6   session_start                      504679 non-null  datetime64[ns, UTC]\n",
      " 7   session_end                        504679 non-null  datetime64[ns, UTC]\n",
      " 8   avg_hour                           504679 non-null  float64            \n",
      " 9   category_code__most_freq_category  504679 non-null  object             \n",
      " 10  brand__most_freq_brand             504679 non-null  object             \n",
      " 11  session_duration_secs              504679 non-null  float64            \n",
      " 12  events_per_minute                  504679 non-null  float64            \n",
      " 13  view_to_cart_ratio                 504679 non-null  float64            \n",
      " 14  cart_to_purchase_ratio             504679 non-null  float64            \n",
      " 15  view_to_purchase_ratio             504679 non-null  float64            \n",
      " 16  final_action                       504679 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](2), float64(8), int64(3), object(4)\n",
      "memory usage: 65.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold TABLE 2 for the Clustering and customer segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               user_id             session_id  total_views  total_carts  \\\n",
      "0  1515915625353226922  1515915625353226922_1            1            0   \n",
      "1  1515915625353230067  1515915625353230067_1            1            0   \n",
      "2  1515915625353230683  1515915625353230683_1            8            0   \n",
      "3  1515915625353230683  1515915625353230683_2            4            0   \n",
      "4  1515915625353230683  1515915625353230683_3            1            0   \n",
      "\n",
      "   total_purchases  avg_price  price_std             session_start  \\\n",
      "0                0    76.4800   0.000000 2020-10-29 11:28:35+00:00   \n",
      "1                0    28.9800   0.000000 2020-10-06 06:30:32+00:00   \n",
      "2                0    22.2725  12.484057 2020-11-09 08:52:51+00:00   \n",
      "3                0    74.2400   0.000000 2020-11-18 10:51:35+00:00   \n",
      "4                0   339.7900   0.000000 2020-12-12 10:33:09+00:00   \n",
      "\n",
      "                session_end  avg_hour category_code__most_freq_category  \\\n",
      "0 2020-10-29 11:28:35+00:00    11.000                electronics.clocks   \n",
      "1 2020-10-06 06:30:32+00:00     6.000                           unknown   \n",
      "2 2020-11-09 09:26:38+00:00     8.875        electronics.audio.acoustic   \n",
      "3 2020-11-18 10:59:26+00:00    10.000        electronics.audio.acoustic   \n",
      "4 2020-12-12 10:33:09+00:00    10.000     computers.peripherals.printer   \n",
      "\n",
      "  brand__most_freq_brand  session_duration_secs  events_per_minute  \\\n",
      "0                  honor                    0.0           1.000000   \n",
      "1                 kester                    0.0           1.000000   \n",
      "2               logitech                 2027.0           0.229995   \n",
      "3               creative                  471.0           0.451977   \n",
      "4                  canon                    0.0           1.000000   \n",
      "\n",
      "   view_to_cart_ratio  cart_to_purchase_ratio  view_to_purchase_ratio  \n",
      "0                 0.0                     0.0                     0.0  \n",
      "1                 0.0                     0.0                     0.0  \n",
      "2                 0.0                     0.0                     0.0  \n",
      "3                 0.0                     0.0                     0.0  \n",
      "4                 0.0                     0.0                     0.0  \n"
     ]
    }
   ],
   "source": [
    "# --- Start by reusing previous session-level aggregation\n",
    "\n",
    "# df_agg was already created in previous step (session-level features)\n",
    "# Now, drop final_action related stuff\n",
    "\n",
    "gold_table_2 = df_agg.copy()\n",
    "\n",
    "# Optional: add user_id back to session table\n",
    "session_user_map = df[['session_id', 'user_id']].drop_duplicates()\n",
    "gold_table_2 = gold_table_2.merge(session_user_map, on='session_id', how='left')\n",
    "\n",
    "# Rearranging columns nicely\n",
    "cols = ['user_id', 'session_id'] + [c for c in gold_table_2.columns if c not in ['user_id', 'session_id']]\n",
    "gold_table_2 = gold_table_2[cols]\n",
    "\n",
    "# Drop final_action because segmentation is unsupervised\n",
    "if 'final_action' in gold_table_2.columns:\n",
    "    gold_table_2.drop('final_action', axis=1, inplace=True)\n",
    "\n",
    "print(gold_table_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               user_id  total_views  total_carts  total_purchases   avg_price  \\\n",
      "0  1515915625353226922            1            0                0   76.480000   \n",
      "1  1515915625353230067            1            0                0   28.980000   \n",
      "2  1515915625353230683           13            0                0  145.434167   \n",
      "3  1515915625353230922            1            0                0  274.400000   \n",
      "4  1515915625353234047           36            0                0  148.662556   \n",
      "\n",
      "   price_std  session_duration_secs  events_per_minute  view_to_cart_ratio  \\\n",
      "0   0.000000               0.000000           1.000000                 0.0   \n",
      "1   0.000000               0.000000           1.000000                 0.0   \n",
      "2   4.161352             832.666667           0.560658                 0.0   \n",
      "3   0.000000               0.000000           1.000000                 0.0   \n",
      "4  10.638771             158.200000           1.257183                 0.0   \n",
      "\n",
      "   cart_to_purchase_ratio  view_to_purchase_ratio  \n",
      "0                     0.0                     0.0  \n",
      "1                     0.0                     0.0  \n",
      "2                     0.0                     0.0  \n",
      "3                     0.0                     0.0  \n",
      "4                     0.0                     0.0  \n"
     ]
    }
   ],
   "source": [
    "# Aggregate sessions to user-level\n",
    "gold_table_2_user = gold_table_2.groupby('user_id').agg({\n",
    "    'total_views': 'sum',\n",
    "    'total_carts': 'sum',\n",
    "    'total_purchases': 'sum',\n",
    "    'avg_price': 'mean',\n",
    "    'price_std': 'mean',\n",
    "    'session_duration_secs': 'mean',\n",
    "    'events_per_minute': 'mean',\n",
    "    'view_to_cart_ratio': 'mean',\n",
    "    'cart_to_purchase_ratio': 'mean',\n",
    "    'view_to_purchase_ratio': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "print(gold_table_2_user.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Class Classification --> Trying to predict customers final action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data\n",
    "X = df_final.drop(['session_id', 'final_action'], axis=1)\n",
    "y = df_final['final_action']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 504679 entries, 0 to 504678\n",
      "Data columns (total 15 columns):\n",
      " #   Column                             Non-Null Count   Dtype              \n",
      "---  ------                             --------------   -----              \n",
      " 0   total_views                        504679 non-null  int64              \n",
      " 1   total_carts                        504679 non-null  int64              \n",
      " 2   total_purchases                    504679 non-null  int64              \n",
      " 3   avg_price                          504679 non-null  float64            \n",
      " 4   price_std                          504679 non-null  float64            \n",
      " 5   session_start                      504679 non-null  datetime64[ns, UTC]\n",
      " 6   session_end                        504679 non-null  datetime64[ns, UTC]\n",
      " 7   avg_hour                           504679 non-null  float64            \n",
      " 8   category_code__most_freq_category  504679 non-null  object             \n",
      " 9   brand__most_freq_brand             504679 non-null  object             \n",
      " 10  session_duration_secs              504679 non-null  float64            \n",
      " 11  events_per_minute                  504679 non-null  float64            \n",
      " 12  view_to_cart_ratio                 504679 non-null  float64            \n",
      " 13  cart_to_purchase_ratio             504679 non-null  float64            \n",
      " 14  view_to_purchase_ratio             504679 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](2), float64(8), int64(3), object(2)\n",
      "memory usage: 57.8+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Preprocessing Pipelines\n",
    "1.->Scale numerical features\n",
    "2.-> One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Identify columns\n",
    "num_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "cat_features = ['category_code__most_freq_category', 'brand__most_freq_brand']  # Categorical from aggregation\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definitions\n",
    "\n",
    "TabNetClassifier | CatBoostClassifier | XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Base models\n",
    "tabnet_model = TabNetClassifier(verbose=0)\n",
    "catboost_model = CatBoostClassifier(verbose=0)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor + Model = Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Pipelines\n",
    "tabnet_pipeline = make_pipeline(preprocessor, tabnet_model)\n",
    "catboost_pipeline = make_pipeline(preprocessor, catboost_model)\n",
    "xgb_pipeline = make_pipeline(preprocessor, xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization using OPTUNA\n",
    "\n",
    "We'll use Optuna to optimize each model separately\n",
    "\n",
    "Include L1 and L2 penalties\n",
    "\n",
    "L1 regularization for feature selection etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['total_views', 'total_carts', 'total_purchases', 'avg_price',\n",
      "       'price_std', 'session_start', 'session_end', 'avg_hour',\n",
      "       'category_code__most_freq_category', 'brand__most_freq_brand',\n",
      "       'session_duration_secs', 'events_per_minute', 'view_to_cart_ratio',\n",
      "       'cart_to_purchase_ratio', 'view_to_purchase_ratio'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/26 21:30:08 INFO mlflow.tracking.fluent: Experiment with name 'TabNet_CatBoost_XGB_Tuning_GPU' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/PADHAI%20MMA%20FALL%20SEM/MMA/Winter%20sem%202/Fatih%20ML%202/mlruns/427491984796624465', creation_time=1745717408887, experiment_id='427491984796624465', last_update_time=1745717408887, lifecycle_stage='active', name='TabNet_CatBoost_XGB_Tuning_GPU', tags={}>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"TabNet_CatBoost_XGB_Tuning_GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabnet classifier with hyperparameter tuning using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mlflow\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def objective_tabnet(trial):\n",
    "    start_time = time.time()\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 16, 32),\n",
    "        \"n_a\": trial.suggest_int(\"n_a\", 16, 32),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 6),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-5, 1e-2, log=True), \n",
    "        \"momentum\": trial.suggest_float(\"momentum\", 0.01, 0.4),\n",
    "        \"optimizer_params\": {\"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)}, \n",
    "        \"verbose\": 0, \n",
    "        \"device_name\": 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    }\n",
    "\n",
    "    model = TabNetClassifier(**params)\n",
    "    pipeline = make_pipeline(preprocessor, model)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec = precision_score(y_val, y_pred, average='macro', zero_division=0) \n",
    "        rec = recall_score(y_val, y_pred, average='macro', zero_division=0)   \n",
    "        f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)       \n",
    "\n",
    "        duration_sec = time.time() - start_time\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1_score\": f1,\n",
    "            \"duration_sec\": duration_sec\n",
    "        })\n",
    "    print(\n",
    "        f\"Trial {trial.number} finished in {duration_sec:.2f} sec | \"\n",
    "        f\"Metrics - Accuracy: {acc:.4f}, Precision: {prec:.4f}, \"\n",
    "        f\"Recall: {rec:.4f}, F1 Score: {f1:.4f}\"\n",
    "    )\n",
    "    return f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mlflow\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch \n",
    "\n",
    "def objective_catboost(trial):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Suggest hyperparameters using Optuna\n",
    "    params = {\n",
    "        \"iterations\": 200,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 5),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 1),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"]),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-3, 1),\n",
    "        \"eval_metric\": \"MultiClass\",\n",
    "        \"loss_function\": \"MultiClass\",\n",
    "        \"task_type\": \"GPU\" if torch.cuda.is_available() else \"CPU\", \n",
    "        \"verbose\": 0 \n",
    "    }\n",
    "\n",
    "    # Create and train the CatBoost model within a pipeline\n",
    "    model = CatBoostClassifier(**params)\n",
    "    # Assuming 'preprocessor' is a defined sklearn ColumnTransformer or similar\n",
    "    pipeline = make_pipeline(preprocessor, model)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on validation set\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec = precision_score(y_val, y_pred, average='macro', zero_division=0) \n",
    "        rec = recall_score(y_val, y_pred, average='macro', zero_division=0)   \n",
    "        f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)       \n",
    "\n",
    "        duration_sec = time.time() - start_time\n",
    "\n",
    "        # Log parameters and metrics to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1_score\": f1,\n",
    "            \"duration_sec\": duration_sec\n",
    "        })\n",
    "\n",
    "    # Print metrics and time taken after the trial completes\n",
    "    print(\n",
    "        f\"Trial {trial.number} finished in {duration_sec:.2f} sec | \"\n",
    "        f\"Metrics - Accuracy: {acc:.4f}, Precision: {prec:.4f}, \"\n",
    "        f\"Recall: {rec:.4f}, F1 Score: {f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Return the metric to be minimized/maximized by Optuna\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mlflow\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def objective_xgb(trial):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    params = {\n",
    "        \"n_estimators\": 200, \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 10.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),\n",
    "        \"tree_method\": \"gpu_hist\", \n",
    "        \"use_label_encoder\": False, \n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"random_state\": 42 \n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    pipeline = make_pipeline(preprocessor, model)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Fit the pipeline\n",
    "        # Use early_stopping_rounds if you have a separate evaluation set for early stopping\n",
    "        pipeline.fit(X_train, y_train, xgbclassifier__eval_set=[(X_val, y_val)], xgbclassifier__early_stopping_rounds=50)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        # Predict on validation set\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec = precision_score(y_val, y_pred, average='macro', zero_division=0) \n",
    "        rec = recall_score(y_val, y_pred, average='macro', zero_division=0)   \n",
    "        f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)       \n",
    "\n",
    "        duration_sec = time.time() - start_time\n",
    "\n",
    "        # Log parameters and metrics to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1_score\": f1,\n",
    "            \"duration_sec\": duration_sec\n",
    "        })\n",
    "\n",
    "    print(\n",
    "        f\"Trial {trial.number} finished in {duration_sec:.2f} sec | \"\n",
    "        f\"Metrics - Accuracy: {acc:.4f}, Precision: {prec:.4f}, \"\n",
    "        f\"Recall: {rec:.4f}, F1 Score: {f1:.4f}\"\n",
    "    )\n",
    "    return f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 22:29:02,253] A new study created in memory with name: TabNet_Tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tuning TabNet \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\AppData\\Roaming\\Python\\Python313\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Optuna study settings\n",
    "n_trials = 30\n",
    "\n",
    "# ========== Run TabNet ==========\n",
    "print(\" Tuning TabNet \")\n",
    "\n",
    "study_tabnet = optuna.create_study(direction=\"maximize\", study_name=\"TabNet_Tuning\")\n",
    "study_tabnet.optimize(objective_tabnet, n_trials=n_trials)\n",
    "\n",
    "print(f\" Best TabNet Score: {study_tabnet.best_value:.4f}\")\n",
    "print(f\" Best TabNet Params: {study_tabnet.best_params}\")\n",
    "\n",
    "# ========== Run CatBoost ==========\n",
    "print(\" Tuning CatBoost \")\n",
    "\n",
    "study_catboost = optuna.create_study(direction=\"maximize\", study_name=\"CatBoost_Tuning\")\n",
    "study_catboost.optimize(objective_catboost, n_trials=n_trials)\n",
    "\n",
    "print(f\" Best CatBoost Score: {study_catboost.best_value:.4f}\")\n",
    "print(f\" Best CatBoost Params: {study_catboost.best_params}\")\n",
    "\n",
    "# ========== Run XGBoost ==========\n",
    "print(\" Tuning XGBoost \")\n",
    "\n",
    "study_xgb = optuna.create_study(direction=\"maximize\", study_name=\"XGB_Tuning\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=n_trials)\n",
    "\n",
    "print(f\" Best XGBoost Score: {study_xgb.best_value:.4f}\")\n",
    "print(f\" Best XGBoost Params: {study_xgb.best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "best_params = {\n",
    "    \"tabnet\": study_tabnet.best_params,\n",
    "    \"catboost\": study_catboost.best_params,\n",
    "    \"xgboost\": study_xgb.best_params,\n",
    "}\n",
    "\n",
    "with open(\"best_model_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "\n",
    "print(\" Best hyperparameters saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# Optimization history plots\n",
    "vis.plot_optimization_history(study_tabnet).show()\n",
    "vis.plot_optimization_history(study_catboost).show()\n",
    "vis.plot_optimization_history(study_xgb).show()\n",
    "\n",
    "# Parameter importance plots\n",
    "vis.plot_param_importances(study_tabnet).show()\n",
    "vis.plot_param_importances(study_catboost).show()\n",
    "vis.plot_param_importances(study_xgb).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Final Individual Models (TabNet, CatBoost, XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# TabNet\n",
    "tabnet_model = TabNetClassifier(**best_params['tabnet'])\n",
    "\n",
    "# CatBoost\n",
    "catboost_model = CatBoostClassifier(\n",
    "    **best_params['catboost'], \n",
    "    task_type='GPU', devices='0'\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_params['xgboost'], \n",
    "    tree_method='gpu_hist', predictor='gpu_predictor', gpu_id=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackingClassifier Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('tabnet', tabnet_model),\n",
    "        ('catboost', catboost_model),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    stack_method='predict_proba',  \n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stack_method='predict_proba' helps stacking better for multi-class.\n",
    "\n",
    "\n",
    "passthrough=True sends original features + predictions to final estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function for Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_stacking(trial):\n",
    "    # Suggest C (regularization) for final Logistic Regression\n",
    "    c = trial.suggest_loguniform('final_estimator_C', 1e-5, 10)\n",
    "\n",
    "    stacked_model.final_estimator = LogisticRegression(C=c, max_iter=1000)\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fit\n",
    "        stacked_model.fit(X_train, y_train)\n",
    "\n",
    "        preds = stacked_model.predict(X_val)\n",
    "\n",
    "        acc = accuracy_score(y_val, preds)\n",
    "        precision = precision_score(y_val, preds, average='weighted')\n",
    "        recall = recall_score(y_val, preds, average='weighted')\n",
    "        f1 = f1_score(y_val, preds, average='weighted')\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Log metrics and params\n",
    "        mlflow.log_params({\n",
    "            \"stack_C\": c,\n",
    "        })\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"duration\": duration\n",
    "        })\n",
    "\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna for Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_stacking = optuna.create_study(direction=\"maximize\", study_name=\"StackedModel_Tuning\")\n",
    "study_stacking.optimize(objective_stacking, n_trials=20)\n",
    "\n",
    "print(f\" Best Stacking Score: {study_stacking.best_value:.4f}\")\n",
    "print(f\" Best Stacking Params: {study_stacking.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Final Stacked Model to MLflow Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train on full training set (train + valid)\n",
    "best_c = study_stacking.best_params['final_estimator_C']\n",
    "stacked_model.final_estimator = LogisticRegression(C=best_c, max_iter=1000)\n",
    "stacked_model.fit(X_train, y_train)  # retrain on more data\n",
    "\n",
    "# Log model\n",
    "with mlflow.start_run(run_name=\"Final_Stacked_Model\"):\n",
    "    mlflow.sklearn.log_model(stacked_model, \"stacked_model\", registered_model_name=\"CustomerAction_StackedClassifier\")\n",
    "    print(\" Final Stacked Model saved to MLflow Registry!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds = stacked_model.predict(X_val)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_val, preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "report = classification_report(y_val, preds, output_dict=True)\n",
    "sns.heatmap(pd.DataFrame(report).iloc[:-1, :].T, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "vis.plot_optimization_history(study_stacking).show()\n",
    "vis.plot_param_importances(study_stacking).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ##############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_table_2_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X = gold_table_2_user.drop(columns=[\"user_id\"])\n",
    "\n",
    "# Find Optimal Clusters Using Elbow or Silhouette\n",
    "sil_scores = []\n",
    "k_list = list(range(2, 11))\n",
    "\n",
    "for k in k_list:\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = model.fit_predict(X)\n",
    "    sil = silhouette_score(X, labels)\n",
    "    sil_scores.append(sil)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(k_list, sil_scores, marker='o')\n",
    "plt.xlabel(\"k clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Finding Best k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pick the k with highest silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_final = KMeans(n_clusters=4, random_state=42)\n",
    "user_clusters = kmeans_final.fit_predict(X)\n",
    "\n",
    "gold_table_2_user[\"cluster_label\"] = user_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=30)\n",
    "user_clusters_hdbscan = clusterer.fit_predict(X)\n",
    "\n",
    "gold_table_2_user[\"cluster_label_hdbscan\"] = user_clusters_hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_table_2_user.groupby('cluster_label').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "Cluster 0 → \"Bargain Hunter\"\n",
    "\n",
    "Cluster 1 → \"Browser\"\n",
    "\n",
    "Cluster 2 → \"Big Spender\"\n",
    "\n",
    "Cluster 3 → \"Window Shopper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mapping = {\n",
    "    0: \"Bargain Hunter\",\n",
    "    1: \"Browser\",\n",
    "    2: \"Big Spender\",\n",
    "    3: \"Window Shopper\"\n",
    "}\n",
    "\n",
    "gold_table_2_user[\"customer_segment\"] = gold_table_2_user[\"cluster_label\"].map(cluster_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Classifier to Predict Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Features and target\n",
    "X_seg = gold_table_2_user.drop(columns=[\"user_id\", \"cluster_label\", \"customer_segment\"])\n",
    "y_seg = gold_table_2_user[\"customer_segment\"]\n",
    "\n",
    "# Train/Val Split\n",
    "X_train_seg, X_val_seg, y_train_seg, y_val_seg = train_test_split(X_seg, y_seg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "rf_seg_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_seg_model.fit(X_train_seg, y_train_seg)\n",
    "\n",
    "# Evaluate\n",
    "preds_seg = rf_seg_model.predict(X_val_seg)\n",
    "print(classification_report(y_val_seg, preds_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = silhouette_score(X, user_clusters)\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"CustomerSegmentation\"):\n",
    "    mlflow.log_params({\n",
    "        \"Clustering Method\": \"KMeans\",\n",
    "        \"n_clusters\": 4,\n",
    "    })\n",
    "    mlflow.log_metrics({\n",
    "        \"silhouette_score\": silhouette\n",
    "    })\n",
    "    \n",
    "    mlflow.sklearn.log_model(rf_seg_model, \"SegmentClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=gold_table_2_user['customer_segment'], palette=\"Set2\")\n",
    "plt.title(\"User Segments via PCA\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
